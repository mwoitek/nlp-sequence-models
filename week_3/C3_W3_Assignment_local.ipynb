{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Named Entity Recognition (NER)\n",
    "\n",
    "Welcome to the third programming assignment of Course 3. In this assignment, you\n",
    "will learn to build more complicated models with Trax. By completing this\n",
    "assignment, you will be able to:\n",
    "\n",
    "- Design the architecture of a neural network, train it, and test it.\n",
    "- Process features and represents them\n",
    "- Understand word padding\n",
    "- Implement LSTMs\n",
    "- Test with your own sentence\n",
    "\n",
    "## Outline\n",
    "- [Introduction](#0)\n",
    "- [Part 1:  Exploring the data](#1)\n",
    "    - [1.1  Importing the Data](#1.1)\n",
    "    - [1.2  Data generator](#1.2)\n",
    "\t\t- [Exercise 01](#ex01)\n",
    "- [Part 2:  Building the model](#2)\n",
    "\t- [Exercise 02](#ex02)\n",
    "- [Part 3:  Train the Model ](#3)\n",
    "\t- [Exercise 03](#ex03)\n",
    "- [Part 4:  Compute Accuracy](#4)\n",
    "\t- [Exercise 04](#ex04)\n",
    "- [Part 5:  Testing with your own sentence](#5)\n",
    "\n",
    "<a name=\"0\"></a>\n",
    "# Introduction\n",
    "\n",
    "We first start by defining named entity recognition (NER). NER is a subtask of\n",
    "information extraction that locates and classifies named entities in a text. The\n",
    "named entities could be organizations, persons, locations, times, etc.\n",
    "\n",
    "For example:\n",
    "\n",
    "<img src = 'ner.png' width=\"width\" height=\"height\"\n",
    "style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "Is labeled as follows:\n",
    "\n",
    "- French: geopolitical entity\n",
    "- Morocco: geographic entity\n",
    "- Christmas: time indicator\n",
    "\n",
    "Everything else that is labeled with an `O` is not considered to be a named\n",
    "entity. In this assignment, you will train a named entity recognition system\n",
    "that could be trained in a few seconds (on a GPU) and will get around 75%\n",
    "accuracy. Then, you will load in the exact version of your model, which was\n",
    "trained for a longer period of time. You could then evaluate the trained version\n",
    "of your model to get 96% accuracy! Finally, you will be able to test your named\n",
    "entity recognition system with your own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from utils import get_params, get_vocab\n",
    "\n",
    "# Set random seeds to make this notebook easier to replicate:\n",
    "trax.supervised.trainer_lib.init_random_number_generators(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# Part 1:  Exploring the data\n",
    "\n",
    "We will be using a dataset from Kaggle, which we will preprocess for you. The\n",
    "original data consists of four columns, the sentence number, the word, the part\n",
    "of speech of the word, and the tags.  A few tags you might expect to see are:\n",
    "\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person\n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original Kaggle data:\n",
    "data = pd.read_csv('ner_dataset.csv', encoding='ISO-8859-1')\n",
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "print(f'SENTENCE: {train_sents}')\n",
    "print(f'SENTENCE LABEL: {train_labels}')\n",
    "print('ORIGINAL DATA:')\n",
    "print(data.head(5))\n",
    "del(data, train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.1\"></a>\n",
    "## 1.1  Importing the Data\n",
    "\n",
    "In this part, we will import the preprocessed data and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(\n",
    "    vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt'\n",
    ")\n",
    "v_sentences, v_labels, v_size = get_params(\n",
    "    vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt'\n",
    ")\n",
    "test_sentences, test_labels, test_size = get_params(\n",
    "    vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vocab` is a dictionary that translates a word string to a unique number. Given\n",
    "a sentence, you can represent it as an array of numbers translating with this\n",
    "dictionary. The dictionary contains a `<PAD>` token.\n",
    "\n",
    "When training an LSTM using batches, all your input sentences must be the same\n",
    "size. To accomplish this, you set the length of your sentences to a certain\n",
    "number and add the generic `<PAD>` token to fill all the empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab translates from a word to a unique number:\n",
    "print(f\"vocab['the']: {vocab['the']}\")\n",
    "# <PAD> token:\n",
    "print(f\"<PAD> token: {vocab['<PAD>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag_map corresponds to one of the possible tags a word can have. Run the\n",
    "cell below to see the possible classes you will be predicting. The prepositions\n",
    "in the tags mean:\n",
    "* I: Token is inside an entity.\n",
    "* B: Token begins an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the coding scheme that tags the entities is a minimal one where B- indicates\n",
    "the first token in a multi-token entity, and I- indicates one in the middle of a\n",
    "multi-token entity. If you had the sentence\n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "the outputs would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your tags would reflect three tokens beginning with B-, since there are no\n",
    "multi-token entities in the sequence. But if you added Sharon's last name to the\n",
    "sentence:\n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as\n",
    "I-per, where I- indicates an inner token in a multi-token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring information about the data:\n",
    "print(f'Number of tags in tag_map: {len(tag_map)}')\n",
    "# Number of vocabulary tokens (including <PAD>):\n",
    "g_vocab_size = len(vocab)\n",
    "print(f'Number of vocabulary words: {g_vocab_size}')\n",
    "print(f'Size of the training set: {t_size}')\n",
    "print(f'Size of the validation set: {v_size}')\n",
    "print(f'Example of the first sentence: {t_sentences[0]}')\n",
    "print(f'Corresponding label: {t_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that we have already encoded each sentence into a tensor by\n",
    "converting it into a number. We also have 16 possible classes, as shown in the\n",
    "tag map.\n",
    "\n",
    "\n",
    "<a name=\"1.2\"></a>\n",
    "## 1.2  Data generator\n",
    "\n",
    "In python, a generator is a function that behaves like an iterator. It will\n",
    "return the next item. Here is a [link](https://wiki.python.org/moin/Generators)\n",
    "to review python generators.\n",
    "\n",
    "In many AI applications it is very useful to have a data generator. You will now\n",
    "implement a data generator for our NER application.\n",
    "\n",
    "<a name=\"ex01\"></a>\n",
    "### Exercise 01\n",
    "\n",
    "**Instructions:** Implement a data generator function that takes in `batch_size,\n",
    "x, y, pad, shuffle` where x is a large list of sentences, and y is a list of the\n",
    "tags associated with those sentences and pad is a pad value. Return a subset of\n",
    "those inputs in a tuple of two arrays `(X,Y)`. Each is an array of dimension\n",
    "(`batch_size, max_len`), where `max_len` is the length of the longest sentence\n",
    "*in that batch*. You will pad the X and Y examples with the pad argument. If\n",
    "`shuffle=True`, the data will be traversed in a random form.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "This code as an outer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "...\n",
    "yield((X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which runs continuously in the fashion of generators, pausing when yielding the\n",
    "next values. We will generate a batch_size output on each pass of this loop.\n",
    "\n",
    "It has two inner loops.\n",
    "1. The first stores in temporal lists the data samples to be included in the\n",
    "next batch, and finds the maximum length of the sentences contained in it. By\n",
    "adjusting the length to include only the size of the longest sentence in each\n",
    "batch, overall computation is reduced.\n",
    "\n",
    "2. The second loop moves those inputs from the temporal list into NumPy arrays\n",
    "pre-filled with pad values.\n",
    "\n",
    "There are three slightly out of the ordinary features.\n",
    "1. The first is the use of the NumPy `full` function to fill the NumPy arrays\n",
    "with a pad value. See [full function\n",
    "documentation](https://numpy.org/doc/1.18/reference/generated/numpy.full.html).\n",
    "\n",
    "2. The second is tracking the current location in the incoming lists of\n",
    "sentences. Generators variables hold their values between invocations, so we\n",
    "create an `index` variable, initialize to zero, and increment by one for each\n",
    "sample included in a batch. However, we do not use the `index` to access the\n",
    "positions of the list of sentences directly. Instead, we use it to select one\n",
    "index from a list of indexes. In this way, we can change the order in which we\n",
    "traverse our original list, keeping untouched our original list.\n",
    "\n",
    "3. The third also relates to wrapping. Because `batch_size` and the length of\n",
    "the input lists are not aligned, gathering a batch_size group of inputs may\n",
    "involve wrapping back to the beginning of the input loop. In our approach, it is\n",
    "just enough to reset the `index` to 0. We can re-shuffle the list of indexes to\n",
    "produce different batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    batch_size: Integer, batch size.\n",
    "    x: List containing sentences. Words are represented by integers.\n",
    "    y: List containing tags associated with the sentences.\n",
    "    pad: Integer representing the pad character.\n",
    "    shuffle: True to shuffle the data.\n",
    "    verbose: True to print information during runtime.\n",
    "\n",
    "    Returns:\n",
    "    X: Padded sentences. Array of shape (batch_size, max_len).\n",
    "    Y: Tags associated with the sentences in X. Array of shape (batch_size, max_len).\n",
    "    \"\"\"\n",
    "\n",
    "    # Index for tracking the current position in x and y:\n",
    "    index = 0\n",
    "\n",
    "    # Number of sentences in x:\n",
    "    num_lines = len(x)\n",
    "\n",
    "    # Create a list with the indexes of x that can be shuffled:\n",
    "    lines_index = [*range(num_lines)]\n",
    "\n",
    "    # If 'shuffle' is True, shuffle the line indexes:\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Initialize the list that will contain the raw x data for this batch:\n",
    "        buffer_x = [0] * batch_size\n",
    "        # Initialize the list that will contain the raw y data for this batch:\n",
    "        buffer_y = [0] * batch_size\n",
    "\n",
    "        # Initialize the length of the longest sentence in this batch:\n",
    "        max_len = 0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # If the index is greater or equal to the number of lines in x:\n",
    "            if index >= num_lines:\n",
    "                # Reset the index:\n",
    "                index = 0\n",
    "                # If 'shuffle' is True, shuffle the indexes:\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(lines_index)\n",
    "\n",
    "            # Store the current x value in buffer_x:\n",
    "            buffer_x[i] = x[lines_index[index]]\n",
    "            # Store the current y value in buffer_y:\n",
    "            buffer_y[i] = y[lines_index[index]]\n",
    "\n",
    "            # Length of the current x:\n",
    "            lenx = len(buffer_x[i])\n",
    "            if lenx > max_len:\n",
    "                max_len = lenx\n",
    "\n",
    "            # Increment index:\n",
    "            index += 1\n",
    "\n",
    "        # Initialize X and Y, arrays of shape (batch_size, max_len).\n",
    "        # Initially, all of their values are equal to 'pad'.\n",
    "        X = np.full((batch_size, max_len), pad)\n",
    "        Y = np.full((batch_size, max_len), pad)\n",
    "\n",
    "        # Copy the values in the buffers into X and Y:\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            x_i = buffer_x[i]\n",
    "            y_i = buffer_y[i]\n",
    "\n",
    "            # Go through each word in x_i:\n",
    "            for j in range(len(x_i)):\n",
    "                # Store the word in X:\n",
    "                X[i, j] = x_i[j]\n",
    "                # Store the label in Y:\n",
    "                Y[i, j] = y_i[j]\n",
    "\n",
    "        if verbose:\n",
    "            print(f'index = {index}')\n",
    "\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "mini_sentences = t_sentences[:8]\n",
    "mini_labels = t_labels[:8]\n",
    "dg = data_generator(\n",
    "    batch_size, mini_sentences, mini_labels, vocab['<PAD>'], shuffle=False, verbose=True\n",
    ")\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(f'X1.shape = {X1.shape}')\n",
    "print(f'Y1.shape = {Y1.shape}')\n",
    "print(f'X2.shape = {X2.shape}')\n",
    "print(f'Y2.shape = {Y2.shape}')\n",
    "print()\n",
    "print(f'X1[0, :] =\\n{X1[0, :]}')\n",
    "print(f'Y1[0, :] =\\n{Y1[0, :]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= 5\n",
    "index= 2\n",
    "(5, 30) (5, 30) (5, 30) (5, 30)\n",
    "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
    "    12    13    14     9    15     1    16    17    18    19    20    21\n",
    " 35180 35180 35180 35180 35180 35180]\n",
    " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
    "     1     0     0     0     0     0     2     0     0     0     0     0\n",
    " 35180 35180 35180 35180 35180 35180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "# Part 2:  Building the model\n",
    "\n",
    "You will now implement the model. You will be using Google's TensorFlow. Your\n",
    "model will be able to distinguish the following:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img src = 'ner1.png' width=\"width\" height=\"height\"\n",
    "style=\"width:500px;height:150px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The model architecture will be as follows:\n",
    "\n",
    "<img src = 'ner2.png' width=\"width\" height=\"height\"\n",
    "style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "Concretely:\n",
    "\n",
    "* Use the input tensors you built in your data generator\n",
    "* Feed it into an Embedding layer, to produce more semantic entries\n",
    "* Feed it into an LSTM layer\n",
    "* Run the output through a linear layer\n",
    "* Run the result through a log softmax layer to get the predicted class for each\n",
    "word.\n",
    "\n",
    "Good news! We won't make you implement the LSTM unit drawn above. However, we\n",
    "will ask you to build the model.\n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### Exercise 02\n",
    "\n",
    "**Instructions:** Implement the initialization step and the forward function of\n",
    "your Named Entity Recognition system.\n",
    "Please utilize help function e.g. `help(tl.Dense)` for more information on a\n",
    "layer\n",
    "\n",
    "-\n",
    "[tl.Serial](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26):\n",
    "Combinator that applies layers serially (by function composition).\n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas.\n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...),\n",
    "tl.LogSoftmax(...))`\n",
    "\n",
    "\n",
    "-\n",
    "[tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113):\n",
    "Initializes the embedding. In this case it is the dimension of the model by the\n",
    "size of the vocabulary.\n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices\n",
    "for a word embedding size range from 150 to 300, for example).\n",
    "\n",
    "\n",
    "-\n",
    "[tl.LSTM](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87):`Trax`\n",
    "LSTM layer of size d_model.\n",
    "    - `LSTM(n_units)` Builds an LSTM layer of n_cells.\n",
    "\n",
    "\n",
    "\n",
    "-\n",
    "[tl.Dense](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L28):\n",
    "A dense layer.\n",
    "    - `tl.Dense(n_units)`: The parameter `n_units` is the number of units chosen\n",
    "for this dense layer.\n",
    "\n",
    "\n",
    "-\n",
    "[tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242):\n",
    "Log of the output probabilities.\n",
    "    - Here, you don't need to set any parameters for `LogSoftMax()`.\n",
    "\n",
    "\n",
    "**Online documentation**\n",
    "\n",
    "- [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-\n",
    "trax.layers.combinators)\n",
    "\n",
    "- [tl.Embedding](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "\n",
    "-  [tl.LSTM](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM)\n",
    "\n",
    "-  [tl.Dense](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)\n",
    "\n",
    "- [tl.LogSoftmax](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: NER\n",
    "def NER(vocab_size=35181, d_model=50, tags=tag_map):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    vocab_size: Integer, vocabulary size.\n",
    "    d_model: Integer, embedding dimensions.\n",
    "    tags: Dictionary mapping tags to integers.\n",
    "\n",
    "    Returns:\n",
    "    model: Trax Serial model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = tl.Serial(\n",
    "        # Embedding layer:\n",
    "        tl.Embedding(vocab_size=vocab_size, d_feature=d_model),\n",
    "        # LSTM layer:\n",
    "        tl.LSTM(n_units=d_model),\n",
    "        # Dense layer:\n",
    "        tl.Dense(n_units=len(tags)),\n",
    "        # LogSoftmax layer:\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model:\n",
    "model = NER()\n",
    "# Display the model:\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Serial[\n",
    "  Embedding_35181_50\n",
    "  LSTM_50\n",
    "  Dense_17\n",
    "  LogSoftmax\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "# Part 3:  Train the Model\n",
    "\n",
    "This section will train your model.\n",
    "\n",
    "Before you start, you need to create the data generators for training and\n",
    "validation data. It is important that you mask padding in the loss weights of\n",
    "your data, which can be done using the `id_to_mask` argument of\n",
    "`trax.supervised.inputs.add_loss_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "rnd.seed(33)\n",
    "batch_size = 64\n",
    "\n",
    "# Create training data generator. Mask <PAD> token.\n",
    "train_generator = trax.supervised.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], shuffle=True),\n",
    "    id_to_mask=vocab['<PAD>']\n",
    ")\n",
    "\n",
    "# Create validation data generator. Mask <PAD> token.\n",
    "eval_generator = trax.supervised.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], shuffle=True),\n",
    "    id_to_mask=vocab['<PAD>']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 Training the model\n",
    "\n",
    "You will now write a function that takes in your model and trains it.\n",
    "\n",
    "As you've seen in the previous assignments, you will first create the\n",
    "[TrainTask](https://trax-\n",
    "ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.TrainTask)\n",
    "and [EvalTask](https://trax-\n",
    "ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.EvalTask)\n",
    "using your data generator. Then you will use the `training.Loop` to train your\n",
    "model.\n",
    "\n",
    "<a name=\"ex03\"></a>\n",
    "### Exercise 03\n",
    "\n",
    "**Instructions:** Implement the `train_model` program below to train the neural\n",
    "network above. Here is a list of things you should do:\n",
    "- Create the trainer object by calling\n",
    "[`trax.supervised.training.Loop`](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop)\n",
    "and pass in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- model = [NER](#ex02)\n",
    "- [training task](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask)\n",
    "that uses the train data generator defined in the cell above\n",
    "    - loss_layer =\n",
    "[tl.CrossEntropyLoss()](https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py#L71)\n",
    "    - optimizer =\n",
    "[trax.optimizers.Adam(0.01)](https://github.com/google/trax/blob/03cb32995e83fc1455b0c8d1c81a14e894d0b7e3/trax/optimizers/adam.py#L23)\n",
    "- [evaluation task](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask)\n",
    "that uses the validation data generator defined in the cell above\n",
    "    - metrics for `EvalTask`: `tl.CrossEntropyLoss()` and `tl.Accuracy()`\n",
    "    - in `EvalTask` set `n_eval_batches=10` for better evaluation accuracy\n",
    "- output_dir = output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be using a [cross entropy loss](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss),\n",
    "with an [Adam optimizer](https://trax-\n",
    "ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam).\n",
    "Please read the [trax](https://trax-ml.readthedocs.io/en/latest/trax.html)\n",
    "documentation to get a full understanding. The [trax\n",
    "GitHub](https://github.com/google/trax) also contains some useful information\n",
    "and a link to a colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    NER: Trax Serial model.\n",
    "    train_generator: Generator of training data.\n",
    "    eval_generator: Generator of validation data.\n",
    "    train_steps: Number of training steps.\n",
    "    output_dir: Relative path of the directory to save the model.\n",
    "\n",
    "    Returns:\n",
    "    training_loop: Training loop for the model. Type: trax.supervised.training.Loop.\n",
    "    \"\"\"\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator,\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.01)\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=eval_generator,\n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    "        n_eval_batches=10\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(\n",
    "        NER,\n",
    "        train_task,\n",
    "        eval_task=eval_task,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    training_loop.run(n_steps=train_steps)\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your local machine, you can run this training for 1000 train_steps and get\n",
    "your own model. This training takes about 5 to 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Coursera, we can train for only 100 steps:\n",
    "train_steps = 100\n",
    "# Remove old 'model.pkl.gz' file if it exists:\n",
    "!rm -f 'model/model.pkl.gz'\n",
    "# Train the model:\n",
    "training_loop = train_model(NER(), train_generator, eval_generator, train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output (Approximately)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "Step      1: train CrossEntropyLoss |  2.94375849\n",
    "Step      1: eval  CrossEntropyLoss |  1.93172036\n",
    "Step      1: eval          Accuracy |  0.78727312\n",
    "Step    100: train CrossEntropyLoss |  0.57727730\n",
    "Step    100: eval  CrossEntropyLoss |  0.36356260\n",
    "Step    100: eval          Accuracy |  0.90943187\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value may change between executions, but it must be around 90% of accuracy\n",
    "on train and validations sets, after 100 training steps.\n",
    "\n",
    "We have trained the model longer, and we give you such a trained model. In that\n",
    "way, we ensure you can continue with the rest of the assignment even if you had\n",
    "some troubles up to here, and also we are sure that everybody will get the same\n",
    "outputs for the last example. However, you are free to try your model, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model:\n",
    "model = NER()\n",
    "model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "# Part 4:  Compute Accuracy\n",
    "\n",
    "You will now evaluate in the test set. Previously, you have seen the accuracy on\n",
    "the training set and the validation (noted as eval) set. You will now evaluate\n",
    "on your test set. To get a good evaluation, you will need to create a mask to\n",
    "avoid counting the padding tokens when computing the accuracy.\n",
    "\n",
    "<a name=\"ex04\"></a>\n",
    "### Exercise 04\n",
    "\n",
    "**Instructions:** Write a program that takes in your model and uses it to\n",
    "evaluate on the test set. You should be able to get an accuracy of 95%.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>More Detailed Instructions </b></font>\n",
    "</summary>\n",
    "\n",
    "* *Step 1*: model(sentences) will give you the predicted output.\n",
    "\n",
    "* *Step 2*: Prediction will produce an output with an added dimension. For each\n",
    "sentence, for each word, there will be a vector of probabilities for each tag\n",
    "type. For each sentence,word, you need to pick the maximum valued tag. This will\n",
    "require `np.argmax` and careful use of the `axis` argument.\n",
    "* *Step 3*: Create a mask to prevent counting pad characters. It has the same\n",
    "dimension as output. An example below on matrix comparison provides a hint.\n",
    "* *Step 4*: Compute the accuracy metric by comparing your outputs against your\n",
    "test labels. Take the sum of that and divide by the total number of **unpadded**\n",
    "tokens. Use your mask value to mask the padded tokens. Return the accuracy.\n",
    "</detail>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a comparison involving an array:\n",
    "a = np.array([1, 2, 3, 4])\n",
    "a == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(data_generator(len(test_sentences), test_sentences, test_labels, vocab['<PAD>']))\n",
    "print(f'x.shape = {x.shape}')\n",
    "print(f'y.shape = {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pred = model(x)\n",
    "print(f'type(tmp_pred) = {type(tmp_pred)}')\n",
    "print(f'tmp_pred.shape = {tmp_pred.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model's prediction has 3 axes:\n",
    "- the number of examples\n",
    "- the number of words in each example (padded to be as long as the longest\n",
    "sentence in the batch)\n",
    "- the number of possible targets (the 17 named entity tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: evaluate_prediction\n",
    "def evaluate_prediction(pred, labels, pad):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    pred: Prediction array.\n",
    "    labels: Labels array.\n",
    "    pad: Integer representing the pad character.\n",
    "\n",
    "    Returns:\n",
    "    accuracy: Prediction accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = np.argmax(pred, axis=-1)\n",
    "    print(f'outputs shape: {outputs.shape}')\n",
    "\n",
    "    mask = np.not_equal(labels, pad)\n",
    "    print('mask shape:', mask.shape, 'mask[0][20:30]:', mask[0][20:30])\n",
    "    mask_int = mask.astype(np.int32)\n",
    "\n",
    "    accuracy = np.sum(mask_int * np.equal(outputs, labels, dtype=np.int32)) / np.sum(mask_int)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_prediction(model(x), y, vocab['<PAD>'])\n",
    "print(f'accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output (Approximately)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs shape: (7194, 70)\n",
    "mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False\n",
    "False False False False]\n",
    "accuracy:  0.9543761281155191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "# Part 5:  Testing with your own sentence\n",
    "\n",
    "Below, you can test it out with your own sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function you will be using to test your own sentence.\n",
    "def predict(sentence, model, vocab, tag_map):\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "    batch_data = np.ones((1, len(s)))\n",
    "    batch_data[0][:] = s\n",
    "    sentence = np.array(batch_data).astype(int)\n",
    "    output = model(sentence)\n",
    "    outputs = np.argmax(output, axis=2)\n",
    "    labels = list(tag_map.keys())\n",
    "    pred = []\n",
    "    for i in range(len(outputs[0])):\n",
    "        idx = outputs[0][i]\n",
    "        pred_label = labels[idx]\n",
    "        pred.append(pred_label)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the output for the introduction example\n",
    "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
    "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
    "\n",
    "# New york times news:\n",
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Results **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Peter B-per\n",
    "Navarro, I-per\n",
    "White B-org\n",
    "House I-org\n",
    "Sunday B-tim\n",
    "morning I-tim\n",
    "White B-org\n",
    "House I-org\n",
    "coronavirus B-tim\n",
    "fall, B-tim"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
